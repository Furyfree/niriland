#!/usr/bin/env bash

set -euo pipefail

NIRILAND_ROOT="${NIRILAND_DIR:-$HOME/.local/share/niriland}"

# Allow running from repo directory as fallback
if [[ -d "$PWD/configs/system/etc/systemd/system" ]]; then
  REPO_ROOT="$PWD"
else
  REPO_ROOT="$NIRILAND_ROOT"
fi

SYSTEMD_SRC_DIR="$REPO_ROOT/configs/system/etc/systemd/system"
OPENWEBUI_SERVICE_DST="/etc/systemd/system/openwebui.service"
OPENWEBUI_SERVICE_CPU="$SYSTEMD_SRC_DIR/openwebui.service"
OPENWEBUI_SERVICE_NVIDIA="$SYSTEMD_SRC_DIR/openwebui-nvidia.service"
OPENWEBUI_PORT="3210"
MODEL_NVIDIA="${NIRILAND_OLLAMA_MODEL_NVIDIA:-qwen2.5-coder:14b}"
MODEL_CPU="${NIRILAND_OLLAMA_MODEL_CPU:-qwen2.5-coder:3b}"

log() {
  printf '==> %s\n' "$*"
}

warn() {
  printf 'WARN: %s\n' "$*" >&2
}

die() {
  printf 'ERROR: %s\n' "$*" >&2
  exit 1
}

usage() {
  cat <<'EOF'
Usage: niriland-setup-ai [setup|status|help]

Commands:
  setup   Install and configure Opencode, Codex, Ollama, and OpenWebUI (default)
  status  Show service/tool status
  help    Show this help

Environment:
  NIRILAND_OLLAMA_MODEL_NVIDIA  Model for NVIDIA setup (default qwen2.5-coder:14b)
  NIRILAND_OLLAMA_MODEL_CPU     Model for CPU setup (default qwen2.5-coder:3b)
  NIRILAND_DIR                  Fallback path to the niriland repo
EOF
}

require_cmd() {
  command -v "$1" >/dev/null 2>&1 || die "Missing required command: $1"
}

require_sudo() {
  sudo -v || die "sudo authentication failed"
}

install_package_if_missing() {
  local pkg

  for pkg in "$@"; do
    if pacman -Qq "$pkg" >/dev/null 2>&1; then
      log "$pkg already installed, skipping."
      continue
    fi

    log "Installing $pkg..."
    sudo pacman -S --needed --noconfirm "$pkg"
  done
}

has_nvidia_hardware() {
  if command -v lspci >/dev/null 2>&1 && lspci | grep -qi 'nvidia'; then
    return 0
  fi

  if command -v nvidia-smi >/dev/null 2>&1; then
    return 0
  fi

  return 1
}

is_nvidia_runtime_ready() {
  if ! command -v nvidia-smi >/dev/null 2>&1; then
    return 1
  fi

  # CUDA workloads require the driver to be loaded and nvidia-smi to work.
  nvidia-smi >/dev/null 2>&1
}

install_opencode_if_missing() {
  if command -v opencode >/dev/null 2>&1; then
    log "Opencode already installed, skipping."
    return 0
  fi

  require_cmd curl
  log "Installing Opencode..."
  if curl -fsSL https://opencode.ai/install | bash; then
    log "Opencode installed."
  else
    warn "Opencode installation failed, continuing."
  fi
}

install_codex_if_missing() {
  if command -v codex >/dev/null 2>&1 || [[ -x "$HOME/.local/bin/codex" ]]; then
    log "Codex already installed, skipping."
    return 0
  fi

  install_package_if_missing npm
  log "Installing Codex..."
  if npm i -g @openai/codex --prefix "$HOME/.local"; then
    log "Codex installed."
  else
    warn "Codex installation failed, continuing."
  fi
}

pull_model_if_missing() {
  local model="$1"

  if ollama list 2>/dev/null | awk 'NR > 1 { print $1 }' | grep -Fxq "$model"; then
    log "Model $model already present, skipping."
    return 0
  fi

  log "Pulling model $model..."
  ollama pull "$model"
}

setup_openwebui_service() {
  local service_src="$1"
  local image="$2"

  [[ -f "$service_src" ]] || die "OpenWebUI service template not found: $service_src"

  if sudo docker image inspect "$image" >/dev/null 2>&1; then
    log "OpenWebUI image already present: $image"
  else
    log "Pulling OpenWebUI image: $image"
    sudo docker pull "$image"
  fi

  if sudo docker ps -a --format '{{.Names}}' | grep -Fxq "open-webui"; then
    log "Removing existing open-webui container"
    sudo docker rm -f open-webui >/dev/null
  fi

  sudo install -m 0644 "$service_src" "$OPENWEBUI_SERVICE_DST"
  sudo systemctl daemon-reload
  sudo systemctl enable --now openwebui
  sudo systemctl restart openwebui
}

show_status() {
  local unit

  for unit in ollama docker.service openwebui; do
    if systemctl is-active "$unit" >/dev/null 2>&1; then
      log "$unit is active."
    else
      warn "$unit is not active."
    fi
  done

  if command -v ollama >/dev/null 2>&1; then
    log "Installed Ollama models:"
    ollama list || true
  else
    warn "ollama command not found."
  fi
}

setup_ai() {
  require_cmd pacman
  require_cmd sudo
  require_cmd systemctl
  require_sudo

  [[ -d "$SYSTEMD_SRC_DIR" ]] || die "Could not find service templates under $SYSTEMD_SRC_DIR"

  log "Using repo: $REPO_ROOT"
  log "Setting up local AI tools"
  install_opencode_if_missing
  install_codex_if_missing

  log "Setting up Ollama and Docker"
  install_package_if_missing ollama docker
  sudo systemctl enable --now ollama
  sudo systemctl enable --now docker.service

  if is_nvidia_runtime_ready; then
    log "NVIDIA runtime detected and ready"
    install_package_if_missing nvidia-container-toolkit
    require_cmd nvidia-ctk
    sudo nvidia-ctk runtime configure --runtime=docker
    sudo systemctl restart docker

    pull_model_if_missing "$MODEL_NVIDIA"
    setup_openwebui_service "$OPENWEBUI_SERVICE_NVIDIA" "ghcr.io/open-webui/open-webui:cuda"
  else
    if has_nvidia_hardware; then
      warn "NVIDIA hardware detected, but runtime is not ready. Falling back to CPU image."
      warn "If you expect CUDA, verify drivers/modules and retry after reboot."
    else
      log "Non-NVIDIA system detected"
    fi

    pull_model_if_missing "$MODEL_CPU"
    setup_openwebui_service "$OPENWEBUI_SERVICE_CPU" "ghcr.io/open-webui/open-webui:main"
  fi

  log "AI setup complete."
  log "OpenWebUI URL: http://localhost:$OPENWEBUI_PORT"
}

case "${1:-setup}" in
  setup)
    setup_ai
    ;;
  status)
    show_status
    ;;
  help|--help|-h)
    usage
    ;;
  *)
    usage
    exit 1
    ;;
esac
